{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86f48e-a831-4624-a392-f4cb8e3a50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 — Volatility Target + Features (EURUSD H1)\n",
    "\n",
    "Goal:\n",
    "- Load processed EURUSD hourly data\n",
    "- Build forward realized volatility targets\n",
    "- Build lag/rolling features\n",
    "- Create walk-forward validation splits (no leakage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bdb83-0e5a-4e4d-a262-0487b44ac2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + paths \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name.lower() == \"notebooks\" else Path.cwd()\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "in_path = DATA_PROCESSED / \"eurusd_h1_kaggle.parquet\"\n",
    "print(\"Loading:\", in_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029069f-5b6b-4fb3-9064-3d32be78c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "df = pd.read_parquet(in_path)\n",
    "\n",
    "# Ensure sorted and unique timestamps\n",
    "df = df.sort_values(\"timestamp\").drop_duplicates(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Date range:\", df[\"timestamp\"].min(), \"→\", df[\"timestamp\"].max())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84bbba-0f6c-475c-b35d-0d7f73bbb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define volatility forecasting target\n",
    "\n",
    "We forecast forward realized volatility (RV) over a horizon H hours:\n",
    "\n",
    "- returns: ret_1h (log return)\n",
    "- target at time t: RV_{t,H} = std(ret_{t+1} ... ret_{t+H}) * sqrt(H)\n",
    "\n",
    "We shift by 1 to prevent leakage (we do NOT use ret at time t in the future window).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fad2d-c144-44fa-af32-d2668b130df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build targets for one or more horizons\n",
    "HORIZONS = [24]  # 24 hours = 1 day of hourly bars; later you can add 48, 72, 168\n",
    "\n",
    "for H in HORIZONS:\n",
    "    # std of the NEXT H returns (shift by -1 so window starts at t+1)\n",
    "    fwd_std = df[\"ret_1h\"].shift(-1).rolling(window=H).std()\n",
    "    df[f\"rv_{H}h\"] = fwd_std * np.sqrt(H)\n",
    "\n",
    "df[[\"timestamp\", \"ret_1h\"] + [f\"rv_{H}h\" for H in HORIZONS]].tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440e9ee-57d1-4a2a-bb04-93950071a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build features (past information only)\n",
    "\n",
    "We create:\n",
    "- lagged returns\n",
    "- rolling realized volatility (past)\n",
    "- rolling statistics of absolute returns\n",
    "- time features (hour-of-day, day-of-week)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b7b9b-2589-48b4-8ff9-b44d218a7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "df_feat = df.copy()\n",
    "\n",
    "# --- time features ---\n",
    "df_feat[\"hour\"] = df_feat[\"timestamp\"].dt.hour\n",
    "df_feat[\"dow\"] = df_feat[\"timestamp\"].dt.dayofweek  # 0=Mon\n",
    "\n",
    "# --- lagged returns ---\n",
    "LAGS = [1, 2, 3, 6, 12, 24]\n",
    "for l in LAGS:\n",
    "    df_feat[f\"ret_lag_{l}\"] = df_feat[\"ret_1h\"].shift(l)\n",
    "\n",
    "# --- rolling volatility features (past) ---\n",
    "ROLL_WINDOWS = [6, 12, 24, 72, 168]  # 6h, 12h, 1d, 3d, 1w\n",
    "for w in ROLL_WINDOWS:\n",
    "    df_feat[f\"vol_{w}h\"] = df_feat[\"ret_1h\"].rolling(w).std() * np.sqrt(w)\n",
    "    df_feat[f\"absret_mean_{w}h\"] = df_feat[\"ret_1h\"].abs().rolling(w).mean()\n",
    "    df_feat[f\"absret_max_{w}h\"] = df_feat[\"ret_1h\"].abs().rolling(w).max()\n",
    "\n",
    "# Optional: range-based feature using OHLC (past hour)\n",
    "# (high-low)/close gives a simple intrabar range measure\n",
    "df_feat[\"hl_range\"] = (df_feat[\"high\"] - df_feat[\"low\"]) / df_feat[\"close\"]\n",
    "\n",
    "df_feat[[\"timestamp\",\"ret_1h\",\"hour\",\"dow\",\"vol_24h\",\"hl_range\"]].head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4c127-8de9-4b82-bd32-24fc70dcb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build modeling table \n",
    "TARGET = \"rv_24h\"\n",
    "\n",
    "feature_cols = (\n",
    "    [\"hour\", \"dow\", \"hl_range\"]\n",
    "    + [f\"ret_lag_{l}\" for l in LAGS]\n",
    "    + [f\"vol_{w}h\" for w in ROLL_WINDOWS]\n",
    "    + [f\"absret_mean_{w}h\" for w in ROLL_WINDOWS]\n",
    "    + [f\"absret_max_{w}h\" for w in ROLL_WINDOWS]\n",
    ")\n",
    "\n",
    "model_df = df_feat[[\"timestamp\", TARGET] + feature_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Modeling table shape:\", model_df.shape)\n",
    "print(\"Date range:\", model_df[\"timestamp\"].min(), \"→\", model_df[\"timestamp\"].max())\n",
    "model_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bdf82-c188-4804-8601-389848f0a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Walk-forward splits\n",
    "\n",
    "We create time-based splits:\n",
    "- Train up to a date\n",
    "- Validate on the next chunk\n",
    "\n",
    "This avoids look-ahead bias and matches real trading/risk forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca19383-c246-4efa-9f19-087c2064b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_walk_forward_splits(timestamps, train_months=18, test_months=3, step_months=3):\n",
    "    \"\"\"\n",
    "    Walk-forward splits for tz-aware timestamps (UTC).\n",
    "    Returns list of (train_idx, test_idx).\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(timestamps)\n",
    "\n",
    "    # ✅ Ensure tz-aware in UTC\n",
    "    if ts.dt.tz is None:\n",
    "        ts = ts.dt.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        ts = ts.dt.tz_convert(\"UTC\")\n",
    "\n",
    "    # ✅ Build timezone-aware boundaries\n",
    "    start = ts.min().to_period(\"M\").to_timestamp().tz_localize(\"UTC\")\n",
    "    end   = ts.max().to_period(\"M\").to_timestamp().tz_localize(\"UTC\")\n",
    "\n",
    "    splits = []\n",
    "    current_train_start = start\n",
    "\n",
    "    while True:\n",
    "        train_end = current_train_start + pd.DateOffset(months=train_months)\n",
    "        test_end  = train_end + pd.DateOffset(months=test_months)\n",
    "\n",
    "        if test_end > end:\n",
    "            break\n",
    "\n",
    "        train_mask = (ts >= current_train_start) & (ts < train_end)\n",
    "        test_mask  = (ts >= train_end) & (ts < test_end)\n",
    "\n",
    "        train_idx = np.where(train_mask.to_numpy())[0]\n",
    "        test_idx  = np.where(test_mask.to_numpy())[0]\n",
    "\n",
    "        if len(train_idx) > 0 and len(test_idx) > 0:\n",
    "            splits.append((train_idx, test_idx))\n",
    "\n",
    "        current_train_start = current_train_start + pd.DateOffset(months=step_months)\n",
    "\n",
    "    return splits\n",
    "\n",
    "splits = make_walk_forward_splits(model_df[\"timestamp\"], train_months=18, test_months=3, step_months=3)\n",
    "print(\"Number of splits:\", len(splits))\n",
    "\n",
    "train_idx, test_idx = splits[0]\n",
    "print(\"First split:\")\n",
    "print(\" Train:\", model_df.loc[train_idx, \"timestamp\"].min(), \"→\", model_df.loc[train_idx, \"timestamp\"].max())\n",
    "print(\" Test: \", model_df.loc[test_idx, \"timestamp\"].min(), \"→\", model_df.loc[test_idx, \"timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb7b0a-647c-4d61-b58d-2f082c5e9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_df[\"timestamp\"].dtype)\n",
    "print(model_df[\"timestamp\"].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fa283-e6c5-45bf-986d-6da2432e4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline evaluation (quick)\n",
    "\n",
    "Before ML, test a strong baseline:\n",
    "- Predict future volatility using past 24h volatility (vol_24h)\n",
    "This sets a benchmark to beat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641be1b-e1f7-4315-a6ac-b3af441b59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline evaluation on walk-forward splits\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "TARGET = \"rv_24h\"\n",
    "BASELINE_COL = \"vol_24h\"\n",
    "\n",
    "def eval_over_splits(df, splits, y_col, yhat_col):\n",
    "    rows = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        y_true = df.loc[test_idx, y_col].to_numpy()\n",
    "        y_pred = df.loc[test_idx, yhat_col].to_numpy()\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "        rows.append({\n",
    "            \"split\": i,\n",
    "            \"test_start\": df.loc[test_idx, \"timestamp\"].min(),\n",
    "            \"test_end\": df.loc[test_idx, \"timestamp\"].max(),\n",
    "            \"n_test\": len(test_idx),\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "baseline_results = eval_over_splits(model_df, splits, TARGET, BASELINE_COL)\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492cc25-5583-4fe8-967e-b69ba3745b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary + save artifacts\n",
    "print(\"Baseline summary across splits\")\n",
    "print(\"MAE  mean:\", baseline_results[\"mae\"].mean())\n",
    "print(\"MAE   std:\", baseline_results[\"mae\"].std())\n",
    "print(\"RMSE mean:\", baseline_results[\"rmse\"].mean())\n",
    "print(\"RMSE  std:\", baseline_results[\"rmse\"].std())\n",
    "\n",
    "# Save the modeling table (features + target)\n",
    "out_model_path = DATA_PROCESSED / \"eurusd_h1_model_table_rv24h.parquet\"\n",
    "model_df.to_parquet(out_model_path, index=False)\n",
    "\n",
    "# Save split info (as dates + counts, not raw indices)\n",
    "out_splits_path = DATA_PROCESSED / \"eurusd_h1_walkforward_splits_rv24h.csv\"\n",
    "baseline_results.to_csv(out_splits_path, index=False)\n",
    "\n",
    "print(\"\\nSaved modeling table to:\", out_model_path)\n",
    "print(\"Saved split report to:\", out_splits_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd8855-a2da-4fe7-95e5-9d27958bea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
